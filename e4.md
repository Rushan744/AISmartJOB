Mise en situation 3 (E4)

Développement d’une application intégrant un service d’intelligence artificielle

Application : AI SmartJob - Intelligent Job Matching and CV Analysis Service

Formation Développeur en Intelligence Artificielle
RNCP 37827
Promotion 2023-2024

[Prénom Nom]

---

**SOMMAIRE**

1.  Introduction
2.  Analyse du besoin d’application
    *   Modélisation des données
    *   Modélisation des parcours utilisateurs
    *   Spécifications fonctionnelles
3.  Conception de l’application
    *   Spécifications techniques
    *   Flux de données
    *   Preuve de concept
4.  Coordination du projet
    *   Gestion du projet
    *   Outils de pilotage et de collaboration
5.  Développement de l’application
    *   Environnement de développement
    *   Maquettes
    *   Développement des fonctionnalités
    *   Développement des composants métier
    *   Gestion des droits d’accès
    *   Sécurisation de l’application
    *   Couverture des tests
    *   Documentation technique
6.  Tests automatisés
    *   Outil(s) utilisé(s)
    *   Description de la chaîne d’intégration
    *   Exécution des tests
    *   Documentation
7.  Livraison continue
    *   Définition de la chaîne de livraison
    *   Déclencheurs
    *   Configuration de la chaîne de livraison
    *   Test de la chaîne de livraison
    *   Documentation
8.  Perspectives et améliorations
9.  Conclusions
Annexes

---

**1. Introduction**

This report, "Development of an Application Integrating an Artificial Intelligence Service," serves as a direct continuation of Report E3, which focused on the technological watch and the justification for selecting the Mistral 7B Large Language Model (LLM) for our project. Report E4 marks the transition from the analysis phase to the concrete implementation and development of the "AI SmartJob" application. This document will detail the process of designing, developing, testing, and deploying a comprehensive application that seamlessly integrates an AI service for job matching and CV analysis.

The primary objective of this report is to present how the "AI SmartJob" application has been conceived and built to address the inefficiencies of traditional recruitment processes. It aims to provide an intelligent and automated solution that optimizes the matching of candidates with job opportunities. The system is designed to process CVs in PDF format, extract key information, generate personalized job recommendations, and provide detailed career analyses using the locally deployed Mistral 7B LLM. Furthermore, it aggregates job data from multiple external sources to enrich its database.

We will discuss in detail the application's functional and technical specifications, the architectural design, the development methodologies employed, the comprehensive testing strategies implemented, and the continuous delivery pipeline. This document aims to provide a complete overview of the technical implementation of our intelligent professional matching solution, highlighting its core components, security measures, and future potential. The ultimate goal of "AI SmartJob" is to streamline recruitment, making it more precise and efficient for both job seekers and recruiters, while ensuring data confidentiality through local AI model deployment.

---

**2. Application Needs Analysis**

This section details the comprehensive analysis undertaken to understand the application's requirements, ensuring that the developed solution precisely addresses the needs of its users and stakeholders.

*   **Data Modeling**
    The "AI SmartJob" application relies on a robust and flexible data model to manage both structured information and AI-driven outputs. The SQLite relational database serves as the central repository for operational data, ensuring data integrity, supporting complex queries, and enabling reliable storage of all user- and job-related entities.

    Key entities include:
    *   User – authentication and account data.
    *   Candidate – CV-derived information such as name, email, skills, experience, and location.
    *   Job – job postings with attributes like title, company, location, and description.
    *   Feedback – evaluations provided by users (ratings, comments, or satisfaction indicators), which are persisted in SQLite to support continuous improvement of recommendations and system performance.

    External job offers are gathered through web scraping and API integrations (e.g., Adzuna). These heterogeneous datasets may temporarily be staged in a NoSQL database (MongoDB) for flexible handling of raw and semi-structured data. Ultimately, all collected information is aggregated and normalized in SQLite, which acts as the single source of truth for the application.
    The matching logic has evolved over time. An initial design proposed a dedicated matches table in SQLite with manually calculated compatibility scores. This was later abandoned in favor of a more advanced approach: leveraging the Mistral 7B Large Language Model to generate compatibility scores dynamically. These AI-driven scores are calculated at runtime by comparing candidate CVs against job descriptions, providing nuanced and context-aware matching. Since results are generated on demand, they are not stored in a dedicated matches table, but instead used directly for recommendation and display.
    This approach ensures that Emploi Matching combines the reliability of a structured relational database with the flexibility and intelligence of an LLM-based matching engine.

*   **User Journey Modeling**
    Understanding the user's interaction flow is crucial for designing an intuitive and effective application. The "AI SmartJob" application supports several key user journeys:
    1.  **Authentication**: Users first encounter a login interface. If they are new, the system attempts to create an account. Upon successful authentication (or creation), they gain access to the main application features. This journey emphasizes security and ease of access.
    2.  **CV Upload and Analysis**: Authenticated users can upload their CVs (PDF format only). The application then processes this CV, extracts text, and sends it to the AI service. During this process, loading indicators provide feedback to the user.
    3.  **Viewing Job Recommendations**: After CV analysis, the user is presented with a list of recommended job offers, displayed in a structured table. This journey focuses on delivering personalized and relevant opportunities.
    4.  **Viewing Skill Extraction**: Concurrently with job recommendations, the application displays a visualization (e.g., a bar chart) of key skills extracted from their CV, along with a relevance score for each. This provides immediate insight into their professional profile.
    5.  **Receiving Career Analysis**: A detailed text-based career recommendation is provided, offering insights into strengths, areas for improvement, and potential career paths based on their CV content.
    6.  **Submitting Feedback**: Users can provide feedback on the application's performance and utility through a dedicated form, including a star rating and optional comments. This journey is vital for continuous improvement.
    Each journey is designed to be seamless, with clear visual feedback and error handling, adhering to usability standards.

*   **Functional Specifications**
    The "AI SmartJob" application provides a rich set of functionalities, categorized as follows:
    *   **Data Retrieval (ETL)**:
        *   Ability to retrieve a comprehensive list of all job offers stored in the database.
        *   Ability to retrieve a list of all registered candidate profiles.
        *   Ability to retrieve all calculated matches between jobs and candidates.
    *   **User Management**:
        *   Functionality to create new user accounts with secure password hashing.
        *   Controlled access to view a list of all registered users (restricted to administrator accounts).
    *   **AI-Powered Features**:
        *   Generate personalized job recommendations for a specific candidate based on their stored profile information.
        *   Process uploaded PDF CVs to generate tailored job recommendations and a detailed career orientation text.
        *   Extract key skills from a PDF CV and assign a relevance score (0-100) to each.
    *   **User Interaction**:
        *   Allow authenticated users to submit feedback (rating and optional comment) on the application's performance.
    *   **Frontend Serving**:
        *   Serve the main HTML page of the web application, providing the user interface for all interactions.
    These specifications ensure that the application meets the core requirements of intelligent job matching and user interaction, while maintaining data integrity and security.

---

**3. Application Design**

This section outlines the technical framework and architectural choices that underpin the "AI SmartJob" application, ensuring its robustness, scalability, and maintainability.

*   **Technical Specifications**
    The backend of the "AI SmartJob" application is built using **Python** with the **FastAPI** framework. FastAPI was chosen for its high performance, asynchronous capabilities, and automatic generation of interactive API documentation (Swagger UI).
    The core intelligence is powered by the **Mistral 7B Large Language Model (LLM)**, which is deployed and run locally using **Ollama**. This local deployment ensures data privacy and minimizes latency for AI inference. Advanced **prompt engineering** techniques are applied to optimize interactions with the LLM, ensuring precise and contextualized job recommendations and reliable skill extraction.
    For data persistence, a dual-database strategy is employed:
    *   **SQLAlchemy** is used as the Object-Relational Mapper (ORM) for interacting with **SQLite**, which serves as the lightweight relational database for structured application data (users, jobs, candidates, matches, feedback).
    *   **PyMongo** is used to interact with **MongoDB**, a NoSQL database, for storing large volumes of raw, unstructured data from external job offers.
    External data integration is handled by standard Python libraries: `requests` for making HTTP requests to external APIs (e.g., Adzuna) and `BeautifulSoup4` for web scraping job descriptions from websites. PDF content processing is managed by `PyPDF2`.
    Security measures include **Passlib** for robust password hashing (bcrypt) and **HTTP Basic Authentication** for API access control. The entire application is designed to be containerized using Docker for consistent deployment environments.

*   **Data Flow**
    The application's data flow is designed to be efficient and secure, managing information from various sources to deliver intelligent insights to the user.
    1.  **Job Data Ingestion**: Job offers are sourced from two main channels:
        *   **Web Scraping**: The `scraping` module extracts job details from predefined websites.
        *   **External API**: The `api_jobs` module fetches job data from the Adzuna API.
        These raw job data points are initially stored in MongoDB for flexible handling. A processing step then normalizes and combines this data before inserting it into the SQLite relational database, making it ready for matching and recommendations.
    2.  **Candidate Data Management**: Synthetic candidate data can be generated via the `generate_candidates` module and stored in the SQLite database. Real candidate data, primarily from CV uploads, follows a distinct path.
    3.  **CV Processing and AI Analysis**:
        *   When a user uploads a PDF CV via the frontend, the file content is sent to the FastAPI backend.
        *   The `pdf_extractor` module extracts plain text from the PDF bytes.
        *   This extracted text is then passed to the `ai_recommender` module.
        *   The `ai_recommender` interacts with the locally running Ollama LLM to perform two main tasks:
            *   Generate a list of recommended jobs by comparing the CV text against all available jobs.
            *   Extract key skills from the CV text and assign relevance scores.
            *   Generate a detailed career recommendation text.
    4.  **Matching and Feedback**:
        *   The `matching` module calculates a compatibility score between individual jobs and candidates, also leveraging the LLM.
        *   User feedback submitted via the frontend is captured by the FastAPI API and stored in the SQLite database.
    5.  **Frontend Interaction**: The FastAPI backend serves the static frontend files (HTML, CSS, JavaScript). The JavaScript code makes asynchronous API calls to the backend to fetch data, send CVs, and receive AI-generated results, dynamically updating the user interface.

*   **Proof of Concept**
    The "AI SmartJob" application itself stands as a robust proof of concept for integrating an AI service into a practical application. The successful implementation demonstrates:
    *   The feasibility of extracting meaningful text from diverse PDF CVs.
    *   The capability of the locally deployed Mistral 7B LLM to generate relevant job recommendations and insightful career analyses.
    *   The effectiveness of prompt engineering in guiding the LLM to produce structured and accurate outputs (e.g., numbered job lists, JSON-formatted skills).
    *   The seamless integration of the AI backend with a user-friendly web frontend, providing a complete end-to-end solution.
    *   The ability to manage and process data from multiple sources (web scraping, external APIs, user uploads) within a unified system.
    This proof of concept validates the architectural choices and the technical approach, confirming that the core objectives of the project are achievable and functional.

---

**4. Project Coordination**

This section details the methodologies and tools employed to coordinate the technical realization of the "AI SmartJob" application, emphasizing agile practices and an MLOps context to achieve production and quality objectives.

*   **Project Management**
    The project adopted an **agile development methodology**, characterized by iterative cycles and continuous adaptation. This approach facilitated flexibility in responding to evolving requirements and ensured regular delivery of functional increments. Key aspects included:
    *   **Short Development Cycles**: Work was organized into sprints, allowing for focused development and frequent reviews.
    *   **Regular Rituals**: Daily stand-ups ensured team alignment and quick problem resolution. Sprint reviews provided opportunities for stakeholders to inspect progress and provide feedback. Sprint retrospectives fostered continuous improvement of the development process itself.
    *   **MLOps Context**: Given the integration of an AI model, the project adhered to MLOps principles. This involved treating the AI model as a core software component, ensuring its versioning, testing, deployment, and monitoring were integrated into the overall software development lifecycle. This approach aims to bridge the gap between machine learning development and operations, ensuring reliable and scalable AI services.

*   **Management and Collaboration Tools**
    Effective project coordination relies on robust tools that facilitate communication, task tracking, and code management.
    *   **GitHub**: Served as the central platform for version control, hosting the project's codebase. It enabled collaborative development through features like pull requests, code reviews, and issue tracking. Crucially, GitHub Actions were leveraged for Continuous Integration/Continuous Delivery (CI/CD), automating testing and deployment workflows.
    *   **Visual Studio Code (VS Code)**: Was the primary Integrated Development Environment (IDE) for all development activities. Its rich ecosystem of extensions, integrated debugging capabilities, and seamless Git integration streamlined the development workflow, enhancing developer productivity and code quality.
    *   **Communication Platforms**: (If applicable, mention tools like Slack, Microsoft Teams, or Discord for real-time communication and discussions).
    These tools collectively ensured transparency, traceability, and efficient collaboration throughout the project lifecycle.

---

**5. Application Development**

This section details the process of developing the technical components and interfaces of the "AI SmartJob" application, adhering to functional and technical specifications, accessibility standards, security norms, and data management best practices.

*   **Development Environment**
    The development environment was meticulously set up to ensure consistency, efficiency, and reproducibility.
    *   **Core Technologies**: Python 3.11 was the chosen programming language, with FastAPI as the high-performance web framework for the backend.
    *   **Integrated Development Environment (IDE)**: Visual Studio Code (VS Code) was used, providing a comprehensive suite of tools for coding, debugging, and version control integration.
    *   **Containerization**: Docker and Docker Compose were fundamental for creating isolated and consistent development, testing, and deployment environments. This ensured that the application and its dependencies behaved identically across different machines.
    *   **Local AI Model**: The Mistral 7B LLM was run locally via Ollama, allowing for rapid iteration and testing of AI functionalities without external API dependencies, and ensuring data privacy.
    *   **Local Databases**: Both MongoDB (for raw job data) and SQLite (for application data) were run locally, providing direct control and flexibility during development.
    *   **Dependency Management**: Python packages were managed via `requirements.txt`, ensuring all necessary libraries were consistently installed.
    *   **Configuration Management**: Sensitive parameters, such as API keys and service URLs, were securely managed using environment variables, preventing their exposure in the codebase.

*   **Mock-ups**
    While formal, separate mock-up documents might not have been created, the frontend's `index.html` file, along with its associated CSS and JavaScript, effectively served as the application's visual mock-up and implemented user interface. This iterative approach allowed for direct development and refinement of the user experience. The design focused on a clean, intuitive layout using Bootstrap for responsiveness, ensuring accessibility and a consistent look and feel across devices. Key UI elements included sections for CV upload, displaying job recommendations in a tabular format, visualizing extracted skills with interactive charts (using Plotly.js), and a user feedback form.

*   **Feature Development**
    Development focused on building robust backend functionalities and a responsive frontend.
    *   **Backend Development**: Involved creating FastAPI endpoints for user authentication, data retrieval (jobs, candidates, matches), and the core AI services (CV analysis, recommendations, skill extraction, feedback). Emphasis was placed on designing RESTful APIs for clear resource management and efficient communication.
    *   **Frontend Development**: Focused on building the single-page application using HTML, CSS (Bootstrap), and JavaScript. This included implementing client-side logic for user login, handling file uploads, making asynchronous calls to the FastAPI backend, and dynamically rendering the AI-generated results and other data on the page.

*   **Business Component Development**
    The core intelligence and domain-specific logic were encapsulated in dedicated Python modules:
    *   `ai_recommender.py`: Implements the logic for interacting with the LLM to generate job recommendations (both from candidate profiles and CVs) and to provide detailed career analysis. It also handles the extraction of skills with relevance scores.
    *   `matching.py`: Contains the algorithm for calculating a numerical matching score between a single job and a single candidate, leveraging the LLM for nuanced compatibility assessment.
    *   `pdf_extractor.py`: Provides the utility function to reliably extract text content from various PDF file formats, crucial for CV processing.
    *   `scraping.py`: Manages the web scraping logic to collect job offer data from predefined online sources.
    *   `api_jobs.py`: Handles the integration with external job board APIs, such as Adzuna, to enrich the pool of available job offers.
    *   `generate_candidates.py`: A utility for generating synthetic candidate data, useful for testing and initial database population.

*   **Access Rights Management**
    Access control was implemented to secure sensitive data and functionalities.
    *   **HTTP Basic Authentication**: The primary mechanism for authenticating users accessing most API endpoints. User credentials (username/password) are securely transmitted and verified against hashed passwords stored in the database.
    *   **Role-Based Access Control**: Specific endpoints, such as retrieving a list of all users (`/users/`), were explicitly restricted to an "admin" role, ensuring that only authorized personnel could access sensitive administrative data. This hierarchical control enhances overall security.

*   **Application Security**
    Security was a paramount concern throughout development, addressing common vulnerabilities.
    *   **Password Hashing**: User passwords are not stored in plain text. Instead, `passlib` with the bcrypt hashing algorithm is used to securely hash and store passwords, protecting against brute-force attacks and data breaches.
    *   **Input Validation**: All user inputs, especially file uploads (CVs), undergo strict validation. For instance, only PDF files are accepted for CV analysis endpoints, mitigating risks associated with uncontrolled file uploads.
    *   **SQL Injection Prevention**: The use of SQLAlchemy as an ORM for database interactions automatically parametrizes queries, effectively preventing SQL injection vulnerabilities.
    *   **Prompt Injection Mitigation**: While LLMs introduce new security considerations, prompt engineering techniques were designed to minimize the risk of malicious prompt injections by structuring inputs and expected outputs.

*   **Test Coverage**
    A comprehensive testing strategy was implemented to ensure the application's robustness and reliability.
    *   **Unit Tests**: Focused on individual functions and modules (e.g., PDF extraction, password hashing, specific AI recommendation logic components).
    *   **Integration Tests**: Verified the interactions between different modules and services, including API endpoint responses under various conditions (success, authentication failures, validation errors).
    *   **Authentication Tests**: Specifically ensured that protected API endpoints correctly enforced valid credentials and handled unauthorized access attempts.
    *   **Business Logic Tests**: Covered the core business components, such as the matching algorithm and the AI recommendation functions, to ensure they produced expected results.

*   **Technical Documentation**
    Documentation was maintained to facilitate understanding, maintenance, and future development.
    *   **API Documentation**: Automatically generated via Swagger UI (`/docs` endpoint), providing interactive and up-to-date documentation for all API endpoints, including parameters, response formats, and error codes.
    *   **Installation Guide**: Clear instructions for setting up the local development environment using Docker Compose, detailing necessary dependencies and configurations.
    *   **Test Guide**: Comprehensive steps for executing the test suite (e.g., using Pytest) and interpreting the results.
    *   **MLflow Tracking**: Documentation on how MLflow is configured and used to track AI model experiments, including logging parameters, metrics, and artifacts, ensuring traceability and supporting continuous improvement.

---

**6. Automated Tests**

This section details the strategy and implementation of automated testing, crucial for maintaining the technical quality of the "AI SmartJob" application throughout its development lifecycle. Automated tests are integrated into the versioning process to ensure reliability and consistency.

*   **Tool(s) Used**
    A suite of tools was employed to achieve comprehensive automated testing:
    *   **Pytest**: This was the primary testing framework in Python, used for writing and executing both unit tests (e.g., for individual functions like PDF text extraction or password hashing) and integration tests (e.g., for API endpoint interactions). Pytest's fixture system facilitated efficient test setup and teardown.
    *   **FastAPI TestClient**: Provided by the FastAPI framework, this HTTP client was instrumental for simulating API requests directly within automated tests. It allowed for programmatic interaction with API endpoints and precise validation of responses (status codes, JSON payloads, headers) without needing to run a live server.
    *   **Postman**: While primarily used for manual and exploratory testing during development, Postman also allowed for quick validation of API endpoints, request parameters, and JSON responses, complementing the automated suite.
    *   **Katalon**: This tool was utilized for broader application-level integration tests, focusing on end-to-end scenarios. Katalon simulated real user interactions with the frontend and verified the complete flow, from the user interface through the API to the backend and AI components, ensuring the overall system functioned as a unified whole.
    *   **MLflow**: Beyond its primary role in experiment tracking, MLflow was also used to log and monitor metrics related to AI model inferences during testing. This allowed for quantitative assessment of model performance (e.g., number of extracted skills, CV text length) and helped in interpreting test results, especially for AI-specific functionalities.

*   **Description of the Integration Chain**
    The automated testing chain covered critical components and functionalities of the application:
    *   **Text Extraction from Documents**: Tests verified the ability to correctly extract text from various PDF documents (valid, empty, image-based, complex layouts). Assertions focused on the extracted textual content and appropriate error handling for unreadable files.
    *   **Skill Extraction with Scores**: Evaluated the LLM's capability to identify and assign relevance scores (0-100) to key skills from different CVs. Integration tests with the running LLM service validated the structure of the JSON response and the validity of the scores.
    *   **Job Recommendations**: Assessed the relevance of the top 3 job recommendations generated by the LLM for various candidate profiles and job sets. Tests also verified the consistency and coherence of the generated career recommendation text.
    *   **Document Upload via API**: Verified the API's handling of file uploads, ensuring that valid PDF files were accepted (HTTP 200 OK) and non-PDF or unreadable PDF files were rejected with appropriate HTTP 400 Bad Request errors and clear messages.
    *   **Authentication and Authorization**: Ensured that protected API endpoints correctly enforced valid credentials and that access restrictions (e.g., admin-only access) were enforced, returning 401 Unauthorized or 403 Forbidden as expected.

*   **Test Execution**
    Automated tests were executed as an integral part of the continuous integration process. Upon every code push to development branches (e.g., `main`, `develop`) and for every pull request, the entire suite of Python tests was automatically triggered. This immediate feedback loop ensured that regressions or quality issues were detected early in the development cycle, maintaining a stable and functional codebase. The tests were configured to run in a consistent environment (e.g., Docker containers) to eliminate "it works on my machine" issues.

*   **Documentation**
    Test-related documentation was maintained to ensure clarity and reproducibility:
    *   **Test Cases**: Detailed descriptions of test scenarios, expected outcomes, and the specific components being tested.
    *   **Test Execution Guide**: Instructions for running the test suite locally and interpreting the results.
    *   **MLflow Logs**: For AI-specific tests, MLflow automatically logged parameters, metrics, and artifacts (like extracted skill lists), providing a traceable history of model performance during testing. This data was crucial for debugging and continuous improvement of the AI components.

---

**7. Continuous Delivery**

This section outlines the continuous delivery (CD) process implemented for the "AI SmartJob" application, leveraging a continuous integration pipeline and automated tools to ensure optimal application delivery. This MLOps-driven approach ensures that new versions of the application and its AI models can be deployed rapidly, reliably, and with consistent quality.

*   **Definition of the Delivery Chain**
    A robust continuous delivery pipeline was established using **GitHub Actions**, adhering to MLOps principles to automate and standardize the application and AI model lifecycle. This pipeline ensures that code changes and model updates are validated, tested, and prepared for deployment efficiently and reliably. The pipeline typically consists of the following stages:
    *   **Python Tests**: This initial stage executes the entire suite of unit and integration tests defined for the API and AI modules. It includes code syntax checks, running Pytest, and validating interactions between backend components.
    *   **Docker Image Build**: Upon successful completion of the tests, this stage builds the Docker image for the FastAPI application and its frontend. Using a Dockerfile ensures that the application is consistently and reproducibly packaged with all its dependencies.
    *   **Deployment (Placeholder)**: This final stage would deploy the containerized application to a pre-production (staging) or production environment. This could involve using cloud container services or orchestration platforms like Kubernetes. This stage often includes manual approval gates for production environments to add an extra layer of security and control.

*   **Triggers**
    The execution of continuous delivery workflows is initiated by specific events configured within the GitHub Actions workflow files:
    *   **Main Development Branch Pushes**: The "Python Tests" workflow is configured to run automatically on every push to key development branches (e.g., `main` or `develop`). This ensures immediate continuous integration: as soon as code changes are committed, tests are executed, allowing for rapid detection of regressions or code quality issues.
    *   **Pull Requests**: The same testing workflow is triggered when a pull request is opened or updated. This validates code quality *before* it is merged into main branches, facilitating code reviews and reducing the risk of introducing bugs.
    *   **Conditional Triggers**: Subsequent stages in the delivery chain (e.g., Docker image build and deployment) are configured with conditional triggers. They only execute if preceding stages (e.g., tests) succeed, ensuring that only code that has passed all quality checks progresses through the pipeline.
    *   **Future Triggers (e.g., Release Tags)**: For production deployments, triggers based on the creation of version tags could be implemented. This would link a specific deployment to a numbered application version, providing precise traceability and version control. Scheduled triggers (cron jobs) could also be used for regular system health checks or automated maintenance tasks.

*   **Configuration of the Delivery Chain**
    The continuous delivery pipeline is declaratively defined in YAML workflow files, stored in the `.github/workflows/` directory of the repository. This approach keeps the configuration under version control, making it transparent and easy to maintain.
    *   **Execution Environment**: Each workflow runs in a pre-configured Ubuntu runner environment provided by GitHub Actions, where necessary dependencies (such as Python and pip) are installed before tests or builds.
    *   **Dependency Consistency**: The pipeline ensures that all required Python packages are installed from the `requirements.txt` file, maintaining consistency between development, testing, and build environments.
    *   **Secure Parameter Management**: Sensitive parameters, such as external API keys (e.g., Adzuna API key), are not stored directly in version control. Instead, they are managed securely as GitHub Secrets, which are injected into the workflow at runtime. During local development, these are typically kept in a `.env` file excluded from version control.
    *   **Flexibility**: The pipeline is designed to be flexible, allowing for the inclusion of additional steps like containerization, deployment to specific environments, or the launching of support services (e.g., mock databases, simulated AI components) depending on the stage. This configuration enables robust automation, secure handling of sensitive information, and provides a solid foundation for future enhancements to the delivery chain.

*   **Testing the Delivery Chain**
    To ensure the reliability and utility of the AI model monitoring system and the overall CI/CD pipeline, an end-to-end validation process was implemented, specifically focusing on tracking and evaluating model inferences.
    *   **Metric Definition Verification**: Confirmed that each AI model execution logged all relevant parameters (e.g., CV text length, input type) and output metrics (e.g., number of extracted skills, relevance scores) in MLflow.
    *   **Data Logging Assurance**: Ensured that MLflow correctly stored parameters, metrics, and artifacts (such as JSON-formatted skill lists), for each inference, guaranteeing complete traceability.
    *   **Dashboard Validation**: Confirmed that the experiment tracking interface in MLflow accurately displayed executions, allowing for easy comparison of performance across different iterations.
    *   **Performance Anomaly Simulation**: Conducted controlled tests with intentionally altered inputs (e.g., very short or noisy CV text) to verify that recorded metrics reflected the expected performance degradation and that these anomalies were clearly visible in MLflow visualizations.
    These targeted tests ensured that the AI monitoring chain—from inference execution to results analysis—functioned as intended, supporting the continuous improvement of model performance and the reliability of the delivery pipeline.

---

**8. Perspectives and Improvements**

The "AI SmartJob" project has successfully established a solid foundation for an intelligent job matching and candidate analysis solution. However, like any technological endeavor, it presents numerous opportunities for continuous evolution and enhancement to further increase its capabilities, performance, and utility.

*   **Robust Production Deployment**: The next crucial step involves transitioning from a development and testing environment to a full-fledged production deployment. This would entail hosting the application on a scalable cloud platform (e.g., AWS, Azure, Google Cloud Platform) to ensure high availability, elastic scalability based on user load, and enhanced resilience. This would also necessitate implementing robust secret management strategies and utilizing managed database solutions suitable for production environments.

*   **Continuous AI Model Improvement**:
    *   **LLM Fine-tuning**: While Mistral 7B is performant, fine-tuning the model on recruitment-specific datasets (e.g., anonymized job descriptions, CVs, and matching outcomes) could significantly enhance the accuracy and relevance of recommendations and skill extraction.
    *   **MLOps Retraining Pipeline**: Establishing a comprehensive MLOps pipeline for automated and regular retraining of the AI model with new data. This pipeline would include data validation, model training, evaluation, versioning of models, and automatic deployment of the best-performing versions.

*   **Enrichment of Data Sources**: Integrating additional job offer sources (e.g., other job board APIs, company career pages) would enrich the application's database, providing candidates with a broader spectrum of opportunities and improving the diversity of recommendations.

*   **Advanced User Features**:
    *   Developing richer user profiles, allowing candidates to specify detailed search preferences, career goals, and track their application history.
    *   Implementing personalized notifications (via email or in-app) to alert users about new job offers matching their criteria or updates to their profiles.
    *   Adding collaboration features for recruiters (e.g., team management, profile sharing).

*   **Advanced Monitoring and Alerts**: Developing specific business metrics to evaluate the model's real-world performance (e.g., recommendation-to-application conversion rate, offer acceptance rate for recommended candidates). Implementing more sophisticated alerts based on detected anomalies in model performance or data drift, ensuring proactive issue resolution.

*   **UX/UI Enhancements**: Refining the frontend user interface to offer improved ergonomics, more interactive visualizations of matches and CV analyses, and a more intuitive navigation experience. This could involve A/B testing different UI elements and gathering user feedback for continuous improvement.

---

**9. Conclusions**

The "AI SmartJob" project stands as a compelling demonstration of how artificial intelligence can fundamentally transform the recruitment landscape, making it a faster, more precise, and more equitable process. Built upon a robust backend developed with FastAPI and adhering to RESTful principles, the system seamlessly integrates the Mistral 7B LLM, deployed locally via Ollama. This architecture enables highly personalized job recommendations and detailed CV analysis, while simultaneously ensuring compliance with data protection regulations like GDPR through its local execution model.

Throughout its development, the project emphasized quality and reliability. Automated unit and integration tests rigorously validated the functionality of each component, from PDF text extraction to API endpoint responses. MLflow provided invaluable visibility into the AI model's performance, offering comprehensive experiment tracking and facilitating continuous improvement. The CI/CD workflow, rooted in MLOps principles, ensured that development, testing, and delivery were part of a unified, automated, and reproducible cycle.

For recruiters, the platform significantly reduces pre-screening time and enhances the relevance of candidate-job matches. For job seekers, it highlights pertinent skills—including transferable skills often overlooked by traditional keyword-based searches—and provides actionable career guidance. This approach prioritizes accuracy, transparency, and respect for privacy, thereby establishing a strong foundation for the ethical and effective use of AI in recruitment—principles that will undoubtedly shape the future of work.

---

**Annexes**

This section will contain supplementary materials that provide deeper technical detail or visual context to the report. These would typically include:

*   **Key Code Snippets**: Concise examples of critical code logic, such as:
    *   A representative unit test function.
    *   Examples of prompt engineering structures used for the LLM.
    *   A simple API endpoint definition.
    *   The Docker command for running the application.
*   **Configuration Files**:
    *   The `docker-compose.yml` file, illustrating the multi-container setup for the application, Prometheus, and Grafana.
    *   The `prometheus.yml` configuration for monitoring.
*   **Diagrams**:
    *   A data flow diagram illustrating the movement of information through the system.
    *   An architectural diagram showing the interaction between different services (frontend, backend, LLM, databases, monitoring).
*   **Screenshots**:
    *   Screenshots of the application's user interface (e.g., CV upload, recommendations display, skills chart).
    *   Screenshots of monitoring dashboards (e.g., Grafana, MLflow UI).
*   **References**:
    *   A bibliography of external resources, research papers, or documentation referenced in the report.
    *   Links to relevant open-source projects or libraries used.
*   **Data Samples**:
    *   Anonymized examples of input data (e.g., a snippet from `candidats.csv`).
    *   Examples of AI model outputs (e.g., a sample of extracted skills JSON).

These annexes serve to provide concrete evidence and detailed context, supporting the general descriptions provided in the main body of the report.
